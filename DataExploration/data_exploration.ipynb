{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{},"source":["**Importing all The necessary Libraries for performing EDA**"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["# import numpy as np\n","import pandas as pd\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# # import plotly.graph_objects as go\n","# %matplotlib inline\n","# from wordcloud import WordCloud, STOPWORDS"]},{"cell_type":"markdown","metadata":{},"source":["**Reading the file**"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>problem</th>\n","      <th>solution</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>The construction industry is indubitably one o...</td>\n","      <td>Herein, we propose an innovative approach to m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>I'm sure you, like me, are feeling the heat - ...</td>\n","      <td>Imagine standing on a green hill, not a single...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>The massive shift in student learning towards ...</td>\n","      <td>Implement a \"\"Book Swap\"\" program within educa...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>The fashion industry is one of the top contrib...</td>\n","      <td>The proposed solution is a garment rental serv...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>The majority of the materials used in producin...</td>\n","      <td>An innovative concept would be a modular elect...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                            problem  \\\n","0   1  The construction industry is indubitably one o...   \n","1   2  I'm sure you, like me, are feeling the heat - ...   \n","2   3  The massive shift in student learning towards ...   \n","3   4  The fashion industry is one of the top contrib...   \n","4   5  The majority of the materials used in producin...   \n","\n","                                            solution  \n","0  Herein, we propose an innovative approach to m...  \n","1  Imagine standing on a green hill, not a single...  \n","2  Implement a \"\"Book Swap\"\" program within educa...  \n","3  The proposed solution is a garment rental serv...  \n","4  An innovative concept would be a modular elect...  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["DIR_INPUT= 'AI EarthHack Dataset.csv'\n","earthhack_df = pd.read_csv(DIR_INPUT)\n","earthhack_df.head()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The construction industry is indubitably one of the significant contributors to global waste, contributing approximately 1.3 billion tons of waste annually, exerting significant pressure on our landfills and natural resources. Traditional construction methods entail single-use designs that require frequent demolitions, leading to resource depletion and wastage.   \n"]}],"source":["print(earthhack_df['problem'][0])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Herein, we propose an innovative approach to mitigate this problem: Modular Construction. This method embraces recycling and reuse, taking a significant stride towards a circular economy.   Modular construction involves utilizing engineered components in a manufacturing facility that are later assembled on-site. These components are designed for easy disassembling, enabling them to be reused in diverse projects, thus significantly reducing waste and conserving resources.  Not only does this method decrease construction waste by up to 90%, but it also decreases construction time by 30-50%, optimizing both environmental and financial efficiency. This reduction in time corresponds to substantial financial savings for businesses. Moreover, the modular approach allows greater flexibility, adapting to changing needs over time.  We believe, by adopting modular construction, the industry can transit from a 'take, make and dispose' model to a more sustainable 'reduce, reuse, and recycle' model, driving the industry towards a more circular and sustainable future. The feasibility of this concept is already being proven in markets around the globe, indicating its potential for scalability and real-world application.\n"]}],"source":["print(earthhack_df['solution'][0])"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1300, 3)\n","3900\n","Index(['id', 'problem', 'solution'], dtype='object')\n"]}],"source":["print(earthhack_df.shape)\n","print(earthhack_df.size)\n","print(earthhack_df.columns)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Landfills are filling up with post-consumer waste from the textile industry, which consists of non-degradable material, harmful chemicals, and dyes. Additionally, this forms a significant part of the waste that finds its way to the oceans, harming marine life.  \n"]}],"source":["print(earthhack_df['problem'][30])"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The solution is to build an ecosystem around a 'Clothing as a Service' model, which could include a subscription-based clothing rental, an artist platform for clothing customization, and a circular-supply chain powered by textile recycling technologies.   The uniqueness of this solution lies in the added layer of customization and making it into an ecosystem. It combines clothing rental, repair, artist promotion, and textile recycling in one model.  The clothing rental service would work like the previous model, but be subscription-based to provide more flexibility. A points-based currency could also be introduced, which would allow subscribers to avail higher-value items at discounted rates.   An innovation that could set this idea apart is promoting “wearable art” – partnering with local artists to customize clothing items for an additional fee, promoting local art culture, plus bringing uniqueness into the rental catalog.   To make this model circular, post-consumer and post-rental clothes beyond repair could be sent to partnered recycling plants, where they will be treated and turned back into raw textile materials to produce new clothes.   This enhanced model provides significant environmental benefits by reducing textile waste generation, promoting reuse and recycling of clothes. Customizations and recycling efforts can create additional revenue streams while supporting local artists and recycling industries.   As for feasibility and scalability, digital platforms and logistics companies can make this operation feasible on a large scale, while partnerships with artists and recycling factories can enable further expansion. A well-implemented model that addresses potential concerns about cleanliness, quality, and variety, as well as a positive marketing campaign showcasing the environmental and financial benefits to consumers, can increase its acceptability. Hence, the solution is not only feasible and scalable, but it also improves community engagement and the green credentials of the fashion industry.\n"]}],"source":["print(earthhack_df['solution'][30])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# valid_df = pd.read_csv(DIR_INPUT + '/validation.csv')\n","# valid_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# per_lang = valid_df['lang'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sns.barplot(x=per_lang.index, y=per_lang.values)"]},{"cell_type":"markdown","metadata":{},"source":["From validation data the frequency of the lang is in that way as above visuaization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# valid_df.toxic.value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sns.barplot(x=['Not-toxic', 'Toxic'], y=valid_df.toxic.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# per_lang = valid_df.groupby(by=['lang', 'toxic']).count()[['id']]\n","# per_lang"]},{"cell_type":"markdown","metadata":{},"source":["This table explains that the number of toxic and non-toxic comments for each and every lang"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# data=[]\n","# for lang in valid_df['lang'].unique():\n","#       y = per_lang[per_lang.index.get_level_values('lang') == lang].values.flatten()\n","#       data.append(go.Bar(name=lang, x=['Non-toxic', 'Toxic'], y=y))\n","# fig = go.Figure(data=data)\n","# fig.update_layout(\n","#     title='Language distribution in the validation dataset',\n","#     barmode='group'\n","# )\n","# fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_df = pd.read_csv(DIR_INPUT + '/test.csv')\n","# test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_df['lang'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# per_lang = test_df['lang'].value_counts()\n","# sns.barplot(x=per_lang.index, y=per_lang.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["rnd_comments = train_df[train_df['toxic'] == 0].sample(n=10000)['comment_text'].values\n","# wc = WordCloud(background_color=\"black\", max_words=2000, stopwords=STOPWORDS.update(['Trump', 'people', 'one', 'will']))\n","# wc.generate(\" \".join(rnd_comments))\n","\n","# plt.figure(figsize=(20,10))\n","# plt.axis(\"off\")\n","# plt.title(\"Frequent words in non-toxic comments\", fontsize=20)\n","# plt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["From the above word Cloud , we can see the words displayed with more frequency. These are the words found frequently in Non-toxic Comments. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# rnd_comments = train_df[train_df['toxic'] == 1].sample(n=10000)['comment_text'].values\n","# wc = WordCloud(background_color=\"black\", max_words=2000, stopwords=STOPWORDS.update(['Trump', 'people', 'one', 'will']))\n","# wc.generate(\" \".join(rnd_comments))\n","\n","# plt.figure(figsize=(20,10))\n","# plt.axis(\"off\")\n","# plt.title(\"Frequent words in toxic comments\", fontsize=20)\n","# plt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["From the above word Cloud , we can see the words displayed with more frequency. These are the words found frequently in Toxic Comments. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_df.groupby('toxic').describe()"]},{"cell_type":"markdown","metadata":{},"source":["# TEXT PROCESSING- **Cleaning the text**"]},{"cell_type":"markdown","metadata":{},"source":["Lets clean the comments by removing stopwords,punctuations,alpha numeric words"]},{"cell_type":"markdown","metadata":{},"source":["Import all the necessary libraries for performing text processing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","import nltk \n","import string \n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stopwords_set= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n","            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n","            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n","            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n","            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n","            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n","            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n","            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n","            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n","            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n","            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n","            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n","            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n","            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n","            'won', \"won't\", 'wouldn', \"wouldn't\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x = stopwords.words(\"english\")\n","stopwords_nltk_en = set(x)\n","stopwords_nltk_en"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["stoplist_combined = set.union(stopwords_set, stopwords_nltk_en)"]},{"cell_type":"markdown","metadata":{},"source":["Cleaning up the text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","from tqdm import tqdm\n","from bs4 import BeautifulSoup\n","def decontracted(phrase):\n","    # specific\n","    phrase = re.sub(r\"won't\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","    return phrase"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def cleanup_text(msg):\n","    No_Punctuation = [char for char in msg if char not in string.punctuation]\n","    sentance = ''.join(No_Punctuation) #joins all the strings\n","    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n","    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n","    sentance = decontracted(sentance)\n","    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n","    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n","    return [word.lower() for word in sentance.split() if word.lower() not in stoplist_combined]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#train_df['imp_words']= train_df['comment_text'].apply(cleanup_text)\n","#test_df['imp_words']=test_df['content'].apply(cleanup_text)\n","#valid_df['imp_words']=valid_df['comment_text'].apply(cleanup_text)"]},{"cell_type":"markdown","metadata":{},"source":["You can also do this, but it takes lot of time ,so i found alternative way which cleans the content faster. \n","But the above method cleans the text very nicely"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(DIR_INPUT + '/test.csv')\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = train_df\n","test = test_df\n","\n","# remove '\\\\n'\n","train['comment_text'] = train['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n","    \n","# remove any text starting with User... \n","train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n","    \n","# remove IP addresses or user IDs\n","train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n","    \n","#remove http links in the text\n","train['comment_text'] = train['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n","\n","\n","# test set\n","# remove '\\\\n'\n","test['content'] = test['content'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n","    \n","# remove any text starting with User... \n","test['content'] = test['content'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n","    \n","# remove IP addresses or user IDs\n","test['content'] = test['content'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n","    \n","#remove http links in the text\n","test['content'] = test['content'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["valid=valid_df\n","# remove '\\\\n'\n","valid['comment_text'] = valid['comment_text'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n","    \n","# remove any text starting with User... \n","valid['comment_text'] = valid['comment_text'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n","    \n","# remove IP addresses or user IDs\n","valid['comment_text'] = valid['comment_text'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n","    \n","#remove http links in the text\n","valid['comment_text'] = valid['comment_text'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train=train.drop('toxic',axis=1)\n","Y_train=train['toxic']\n","X_valid=valid.drop('toxic',axis=1)\n","Y_valid=valid['toxic']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn import preprocessing\n","encoder = preprocessing.LabelEncoder()\n","Y_train = encoder.fit_transform(Y_train)\n","Y_valid  = encoder.fit_transform(Y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Y_valid"]},{"cell_type":"markdown","metadata":{},"source":["# **Using TF-IDF to convert text to vector**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vect = TfidfVectorizer(analyzer='word',  token_pattern=r'\\w{2,}', max_features=5000)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tfidf_fit_tra = tfidf_vect.fit_transform(X_train).toarray()     # Fit-transform of Train data\n","tfidf_fit_tra"]},{"cell_type":"markdown","metadata":{},"source":["# NOW START BUILDING THE MODEL"]},{"cell_type":"markdown","metadata":{},"source":["I will update the model of having high accuracy soon, stay tuned\n","Please upvote if you like it and keep me motivated "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2703900,"sourceId":19018,"sourceType":"competition"}],"dockerImageVersionId":29860,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":4}
